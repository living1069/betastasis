= Pipeline startup and basics =

On servers where the pipeline is installed, you can start it with the command "pipeline". This command starts a customized Matlab instance with access to functions for the analysis of biological data.

On startup, the pipeline automatically loads into memory some genetic information for Homo sapiens. This species specific information is necessary for running computational analyses. If you're interested in running analyses for some other organism, you can change the currently selected organism by typing:
{{{
    >> select_organism('mus musculus', '2009')
    Reading organism data into memory...
}}}

A list of available organisms can be viewed with the command:
{{{
    >> list_organisms
    List of available organisms:
    - Homo sapiens
    - Mus musculus
}}}
	
You should now select Homo sapiens again before proceeding with the tutorial:
{{{
    >> select_organism('homo sapiens', '2009')
}}}

You can see genetic information about the currently selected organism by accessing the global variable "organism":
{{{
    >> organism
    organism =
                 Name: 'Homo sapiens'
              Version: '2009'
          Chromosomes: [1x1 struct]
                Genes: [1x1 struct]
          Transcripts: [1x1 struct]
                Exons: [1x1 struct]
                miRNA: [1x1 struct]
            pre_miRNA: [1x1 struct]
                 SNPs: [1x1 struct]
           Ontologies: [1x1 struct]
}}}

This global variable is also used by many of the pipeline's internal components. This is why it is important to make sure that you have the correct organism selected when performing data analysis. Having said that, the pipeline does perform some sanity checks and will try to notify the user if the current organism does not seem to match with the data.

In this tutorial we're interested in ovarian cancer, so let's try retrieving some raw microarray data first. Here's an example of loading one of the TCGA ovarian cancer microarray data sets (dataset names are case insensitive):
{{{
    >> qset = query('TCGA/OV Affy HT HG U133A raw')
    qset =
          Type: 'Microarray probe intensities'
        Sample: [1x1 struct]
      Resource: {527x1 cell}
      Platform: {527x1 cell}
       Patient: [1x1 struct]
          Misc: [1x1 struct]
}}} 

From the command output, you can see that this data set includes 527 microarray samples produced with the Affymetrix HT HG U133A gene expression microarray. The data structure you're seeing on the screen is known as a "query set". Query sets are used for filtering data sets down to the desired components, and for studying the metadata associated with data sets. Query sets notably do not contain the actual data, but instead only act as handles to the data. This is useful, because it allows you to perform preliminary filtering on huge data sets without loading gigabytes of data from the disk.

Now, let's have a look at what the query set contains. The "sample_id" vector contains a list of IDs for the samples that were hybridized to the microarray in the 527 experiments. For this TCGA data set, the sample IDs match with the original TCGA sample IDs. Here are the first three:
{{{
    >> qset.Sample.ID(1:3)
    ans =
      'TCGA-13-0807-01'
      'TCGA-13-0891-01'
      'TCGA-13-0912-01'
}}}

The "meta" substructure contains clinical patient information associated with the microarray samples:
{{{
    >> qset.Patient
    ans =
           Gender: [527x1 char]
               ID: {527x1 cell}
           Status: {527x1 cell}
     SurvivalTime: [527x1 double]
	          ...
}}}

The "Misc" substructure contains miscellaneous pieces of clinical information that are not supported by the pipeline data model:
{{{
    >> qset.Misc
    ans =
          ADDITIONALCHEMOTHERAPY: {527x1 cell}
           ADDITIONALDRUGTHERAPY: {527x1 cell}
        ADDITIONALHORMONETHERAPY: {527x1 cell}
	                 ...
}}}

These fields can still be used for filtering samples, but the pipeline predicate query language does not support them. Thus, filtering based on these fields must be performed manually.

Now that we know what a query set consists of, we can try filtering our query set by using the predicate query language:
{{{
    >> female_qset = filter_query(qset, 'gender = female')
    female_qset =
          Type: 'Microarray probe intensities'
      Platform: {509x1 cell}
        Sample: [1x1 struct]
      Resource: {509x1 cell}
       Patient: [1x1 struct]
          Misc: [1x1 struct]
}}}

Here we narrowed our query set down to those samples that came from female patients. Now it should be noted that this data set is for ovarian cancer, so what's going on with the 18 samples that aren't from female patients? Well, let's find out by using the helpful function meta_summary():
{{{
    >> meta_summary(qset)
	...
    Patient.Gender:
    - Female (509 items)
    - N/A (18 items)
	...
}}}

From the output of the function, you can see that the dataset contains 509 samples from female patients (as we saw earlier), and 18 samples from patients of unknown gender. You can see the same result by querying for all samples with unknown gender:
{{{
    >> qset = query('TCGA/OV Affy HT HG U133A raw', 'gender = unknown')
    qset =
          Type: 'Microarray probe intensities'
      Platform: {18x1 cell}
        Sample: [1x1 struct]
      Resource: {18x1 cell}
       Patient: [1x1 struct]
          Misc: [1x1 struct]
}}}

As you can see, the dataset contains 18 samples for which the patient gender is not reported in the clinical data. Nonetheless, it is probably a safe bet that these patients are female as well. We're talking about ovarian cancer, after all :)

In order to gain access to the data a query set refers to, the query set must be "realized" with the following command:
{{{
    >> data = realize(qset)
    Progress: 100%
    data =
        Mean: [247899x18 double]
        Meta: [1x1 struct]
}}}
	
A progress indicator will show up on the screen while the data is being retrieved from the storage. For this query set of 18 samples, the retrieval will happen very fast. Retrieving hundreds of microarray samples from the disk will take a couple of seconds, though. You can see this for yourself by realizing the entire microarray data set:
{{{
    >> data = realize(query('TCGA/OV Affy HT HG U133A'))
    Progress: 100%
    data =
        Mean: [247899x527 double]
        Meta: [1x1 struct]
}}}

And there you have it. You can now view the raw probe intensity data in the following fashion:
{{{
    >> data.Mean(1:3, 1:5)
    ans =
     1.0e+03 *
      0.5315    0.3180    0.3050    0.6150    0.2380
      3.2888    1.6508    1.8145    2.8865    1.3953
      6.3203    3.8602    3.8381    5.7050    3.9452
}}}
	
By looking at the realized data structure, you can also see that all of the original metadata from the query set is still available behind the attribute "Meta":
{{{
    >> data.Meta
    ans =
          Type: 'Microarray probe intensities'
      Platform: {527x1 cell}
        Sample: [1x1 struct]
       Patient: [1x1 struct]
          Misc: [1x1 struct]
}}}

Now, let's have a look at some gene expression data and see how query sets can be merged together. First we need two query sets of gene expression data, both from the TCGA project. One of the query sets will be for the glioblastoma gene expression profiles, one for the ovarian cancer profiles:
{{{
    >> qset_gbm = query('TCGA/GBM Affy HT HG U133A gene expression');
    >> qset_ov = query('TCGA/OV Affy HT HG U133A gene expression');
}}}

Since both query sets refer to gene expression data and were generated for the same organism and using the same annotations, the query sets can be merged together in the following fashion:
{{{
    >> merged = query_union(qset_gbm, qset_ov);
}}}

This function can take an arbitrary amount of query sets to merge, but in this example we called it with just two query sets. As the name of the function implies, a union of the two query sets is taken. Strictly speaking, the union is taken over the "Resource" field of the two query sets, since resource names uniquely identify pieces of data in the pipeline.

But let's now have a look at the merged query set:
{{{
    >> merged
    merged =
                     Misc: [1x1 struct]
                 Organism: 'Homo sapiens'
                  Patient: [1x1 struct]
                 Platform: {922x1 cell}
                 Resource: {922x1 cell}
                   Sample: [1x1 struct]
      SummarizationMethod: {922x1 cell}
                     Type: 'Gene expression'
}}}

The new query set contains both the 395 glioblastoma profiles and the 527 ovarian cancer profiles, for a total of 922 samples. The merged query set can be realized in the normal fashion, and the data resources will be automatically loaded from the two datasets:
{{{
    >> data = realize(merged)
    Progress: 100%
    data =
      Mean: [29595x922 double]
      Meta: [1x1 struct]
}}}

When analyzing the merged data set, you can use metadata to differentiate between tumor types:
{{{
    >> unique(data.Meta.Patient.TumorType)
    ans =
      '-'
      'Serous Cystadenocarcinoma'
      'Treated primary GBM'
      'Untreated primary (De Nova) GBM'
}}}

The symbol '-' always stands for "unknown" in the metadata.

After filtering and working on datasets, you can store it in the pipeline data store as a fresh copy:
{{{
    >> expr = realize(query('TCGA/GBM Affy HT HG U133A gene expression', 1:10));
    >> expr.Mean = quantilenorm(expr.Mean);
    >> create_dataset('quantile normalized example data', expr)
}}}
	
You can also remove existing datasets:
{{{
    >> remove_dataset('quantile normalized example data')
}}}

Note that all new datasets are fully self-contained and include all of the raw data. This means that if you only want to store the result of a filter query, it is a better idea to simply save() the query set object, since you don't want to make an extra copy of the raw data.

Each dataset is also fully self-contained in terms of metadata. This means that if you wish to share data with another group of researchers, you can simply save the data object, and all metadata will be automatically included:
{{{
    >> expr = realize(query('TCGA/GBM Affy HT HG U133A gene expression', 1:10));
    >> save ~/my_expr_data.mat expr
    >> clear expr
    >> load ~/my_expr_data.mat
    >> expr.Meta
    ans =
                   Type: 'Gene expression'
               Platform: {10x1 cell}
                 Sample: [1x1 struct]
                Patient: [1x1 struct]
                   Misc: [1x1 struct]
    SummarizationMethod: {10x1 cell}
               Organism: 'Homo sapiens'
                Version: 'RefSeq 38'
}}}

If the other research group is also using Matlab, they can then immediately use the data, and have access to the full metadata. When sharing gene expression data, it is of course also a good idea to provide the other group with the list of gene names for identifying the rows in your gene expression data matrix.






= MICROARRAY ANALYSIS - TRANSCRIPTOMIC EXPRESSION =

For Agilent gene expression microarrays:
{{{
    gene_expr = read_agilent_gene_arrays(find_files('.*\.txt'));
    gene_expr.mean = normalize_quantiles(gene_expr.mean);
    gene_expr = pset_summary_median_polish(gene_expr);
}}}

Using custom probesets:
{{{
    >> load /worktmp/pipeline/platforms/affy_ht_hg_u133a/gene_probesets
    >> expr = uarray_expression_rma(data, affy_ht_hg_u133a_gene_probesets)
    expr =
        mean: [29595x191 double]
        meta: [1x1 struct]
}}}

From the resulting data structure, you can see that the raw probe intensities were transformed into gene expression levels for the ~30000 different genes named in the RefSeq annotations (these gene names include non-coding, hypothetical and pseudogenes). The rows of the gene expression matrix correspond to the known genes of the currently selected organism:
{{{
    >> organism.Genes
    ans =
                 Name: {29595x1 cell}
      TranscriptCount: [29595x1 double]
          Transcripts: [29595x31 double]
  		     EntrezID: [29595x1 double]
}}}

In this example, we used RMA summarization to calculate the gene expression levels from the raw probe intensities. The used summarization method and other details are automatically documented in the metadata:
{{{
    >> expr.Meta
    ans = 
                     Type: 'Gene expression'
                 Platform: {191x1 cell}
                   Sample: {191x1 cell}
                  Patient: [1x1 struct]
                     Misc: [1x1 struct]
      SummarizationMethod: 'RMA'
                 Organism: 'Homo sapiens'
                  Version: 'RefSeq 38'
}}}

If a suitable probeset is not available for the genome build that you have selected, you can always build a new probeset. To do this, first load the microarray's probe information into the pipeline:
{{{
    >> load /worktmp/pipeline/platforms/affy_ht_hg_u133a/probes
}}}

Then you can create either gene, transcript or exon probesets by using one of the following commands:
{{{
    >> probesets = create_gene_probesets(affy_ht_hg_u133a_probes);
    >> probesets = create_transcript_probesets(affy_ht_hg_u133a_probes);
    >> probesets = create_exon_probesets(affy_ht_hg_u133a_probes);
}}}

The microarray probes will be mapped against the organism's currently selected transcriptome build, and new probesets will be constructed based on the mappings. All three probesets can be used with the uarray_expression_rma() and related functions.




= MICROARRAY ANALYSIS - ARRAY CGH =

Given a probe definition data structure for a microarray platform, you can generate CGH probesets against a given genome version in the following way:
{{{
    >> load /worktmp/pipeline/platforms/agilent_hg_cgh_244a/probes
    >> cgh_probesets = create_cgh_probesets(agilent_hg_cgh_244a_probes)
    cgh_probesets =
            Type: 'Copy number'
        Organism: 'Homo sapiens'
         Version: 'RefSeq 38'
      Chromosome: [235893x1 double]
          Offset: [235893x1 double]
      ProbeCount: [235893x1 double]
          Probes: [235893x3 double]
}}}

For most commonly used CGH microarray platforms, these CGH probesets have been pre-generated and made available under pipeline/platforms, so you only need to do this step for new and exotic CGH arrays.

Many CGH microarrays have been designed so that each probe has a distinct sequence and is complementary to a distinct genomic position. In such CGH microarrays the "!ProbeCount" field will always have a value of 1. Conversely, this field will have values greater than 1 if multiple probes have an identical sequence and interrogate the same genomic position.

With CGH probesets at hand, raw probe level CGH data can be visualized in a genomics viewer software such as IGV by exporting it as a track:
{{{
    >> tumor = query('gbm agilent cgh 244a hms', 'sample type ~ tumor');
    >> normal = query('gbm agilent cgh 244a hms', 'sample type ~ normal');
    >> [tumor, normal] = paired_samples(tumor, normal, 'Patient');
    >> load /worktmp/pipeline/platforms/agilent_hg_cgh_244a/cgh_probesets;
    >> cgh_logratio_track(realize(tumor), realize(normal), ...
           agilent_hg_cgh_244a_probesets, '~/cgh_probe_tracks.igv');
}}}

This will generate a file that contains IGV scatterplot tracks for every paired CGH sample in the provided dataset. The zero level of the CGH logratios will be normalized, but no segmentation or quantization will be performed.

If you wish to run segmentation on the CGH samples, you can use the function cgh_segment_fast() in the following manner:
{{{
    >> tumor = query('gbm agilent cgh 244a hms', 'sample type ~ tumor');
    >> normal = query('gbm agilent cgh 244a hms', 'sample type ~ normal');
    >> [tumor, normal] = paired_samples(tumor, normal, 'Patient');
    >> tumor = realize(tumor);
    >> normal = realize(normal);
    >> segments = cgh_segment_fast(tumor, normal, cgh_probesets);
    >> cn_seg_to_track(segments, 'gbm_copy_number.seg')
}}}

In the above example, we first split the dataset into two query sets, one containing all tumor samples, and one containing all adjacent normal samples. We then use the function paired_samples() to draw sample pairs from these two groups so that both samples in every pair share the same patient ID. So essentially we generated a list of tumor - adjacent normal sample pairs. We then load the CGH probesets from the disk and run a segmentation algorithm on the raw CGH probe intensity data. Finally we export the calculated segments as a .seg copy number track onto the disk. This track can then be visualized with IGV or some other genomic visualization tool.

You can also tune the segmentation algorithm using a number of optional parameters that are documented under HELP CGH_SEGMENT_FAST.

If you prefer to use the circular binary segmentation algorithm by Olshen et al, you can use the segmentation function cgh_segment_cbs().

When using genomic visualization tools to display copy number tracks, make sure that you have selected the right genome build (usually hg19). The data will look bad if visualized against the wrong genome build.

You can also search for statistically significant copy number alterations:
{{{
    >> cna_significance_track(segments, agilent_hg_cgh_244a_probesets, ...
           'significant_cnv.igv')
}}}

This function will also export an IGV track, but this time the track will consist of p-values transformed as log10(1 / p). In other words, the track will contain high peaks where statistically significant copy number alterations are present.

If you're more interested in the percentage-wise recurrence of copy number alterations, you can use another function:
{{{
    >> cna_recurrence_track(segments, agilent_hg_cgh_244a_probesets, ...
           'cnv_recurrence.igv')
}}}







= MICROARRAY ANALYSIS - MICRORNA EXPRESSION =

Given a probe definition data structure for a microarray platform, you can generate microRNA probesets against the currently selected microRNA annotations (see `organism.miRNA`) in the following manner:
{{{
    >> load /worktmp/pipeline/platforms/agilent_human_mirna_8x15k_v2/probes;
    >> mirna_probesets = create_mirna_probesets(agilent_human_mirna_8x15k_v2_probes)
    mirna_probesets =
             miRNA: {904x1 cell}
        ProbeCount: [904x1 double]
            Probes: [904x17 double]
          Organism: 'Homo sapiens'
              Type: 'miRNA expression'
}}}

For most commonly used microRNA arrays, these probesets have been pre-generated and made available under pipeline/platforms, so you only need to do this step for new and exotic miRNA arrays.

Given microRNA probesets, you can calculate microRNA expression levels from raw microarray samples in the following way (note that in this example we use the pre-built probesets):
{{{
    >> load /worktmp/pipeline/platforms/agilent_human_mirna_8x15k_v2/probesets;
    >> raw = realize(query('taylor*mirna raw'));
    >> mirna_expr = uarray_expression_rma(raw, agilent_human_mirna_8x15k_v2_probesets)
    mirna_expr =
        Mean: [904x142 double]
        Meta: [1x1 struct]
}}}

The raw microRNA array samples in this example were loaded from an existing dataset. If your raw samples have not yet been imported as a dataset, you can do so using `import_uarray_data()` (see HELP IMPORT_UARRAY_DATA for more details). 

The rows of the `mirna_expr.Mean` matrix represent different mature miRNAs annotated for the currently selected organism. The rows are in the same order as the fields under `organism.miRNA`. Columns represent samples.




= MICROARRAY ANALYSIS - MISCELLANEOUS =

You can visualize the spatial distribution of probe intensities on the microarray slide by using the function render_uarray_intensities():
{{{
    >> raw = realize(query('TCGA/GBM Affy HT HG U133A raw', 1:10));
    >> render_uarray_intensities(raw, affy_ht_hg_u133a_probes, '~/fig_spatial');
}}}

Many microarray platforms contain so called control probes whose intensity values are not always kept in the raw sample files. So if your microarray data contains !NaN values, you can choose to either visualize those probes as either black or red points. By default those probes will be shown as black, but you can use the optional argument 'RedMissing' to visualize the missing probes as red pixels.

If a visual inspection of the spatially rendered probe intensities indicates that the microarray samples contain significant spatial artifacts, you can apply spatial detrending in the following fashion:
{{{
    >> raw = realize(query('TCGA/GBM Affy HT HG U133A raw', 1:10));
    >> load /worktmp/pipeline/platforms/affy_ht_hg_u133a/probes;
    >> raw = spatial_detrend(raw, affy_ht_hg_u133a_probes);
}}}

You can also visualize the probe intensity histogram for a microarray sample:
{{{
    >> uarray_intensity_hist(raw, '~/histogram');
}}}

Or you can compare the probe composition of two microarray platforms:
{{{
    >> load agilent_human_mirna_8x15k/probes
    >> load agilent_human_mirna_8x15k_v2/probes
    >> compare_probes(agilent_human_mirna_8x15k_v2_probes, ...
           agilent_human_mirna_8x15k_probes);
    Probe comparison results:
    - 7120 probes added
    - 4067 probes removed
    - 7013 probes changed position
}}}




= TRANSCRIPTOME SEQUENCING ANALYSIS =

Now, let's look at some of the pipeline's tools for sequence data analysis. First we need to need to retrieve a set of RNA-seq reads from the pipeline data store:
{{{
    >> reads = realize(query('Wei GBM/WT RNAseq reads', 'sample type ~ GBM'))
    reads =
      SequenceResource: {1x4 cell}
                  Meta: [1x1 struct]
}}}

This example data set consists of whole transcriptome RNA-seq reads from the ABI SOLiD 3 platform. The reads are in colorspace, but the pipeline handles that transparently for you. We can now construct a gene expression profile based on the RNA-seq reads:
{{{
    >> expr = gene_expression_rnaseq(reads, 2, 'RPKM')
    expr =
      Mean: [29595x4 double]
      Meta: [1x1 struct]
}}}

The second argument allows you to select the maximum amount of mismatches allowed while aligning reads to the transcriptome. The third argument lets you select the desired normalization method (the default is to use no normalization). As an optional fourth argument, you can specify a trim length to which all the RNA-seq reads will be trimmed, starting from their 5' end.

If you want to see how the sequenced reads are distributed across transcripts, you can use the following function:
{{{
    >> analyze_seq_read_distribution(reads, '~/read_distribution.pdf');
}}}

The function will create a histogram image that shows the read alignment offsets, normalized by the length of the transcript to which they aligned. By running this function separately on reads sequenced using DGE and RNA-seq  clearly shows the difference between the two sequencing protocols.

If you wish to visualize the density of reads aligning to different regions of the genome, you can generate IGV tracks and then import them into any genomics visualization tool. This is achieved in the following way:
{{{
    >> reads_to_igv(reads, 'alignment_track');
}}}

This will generate four IGV files from the four read files. The names of the generated IGV files are based on the filename prefix given as the second argument. See HELP READS_TO_IGV for more details.

In addition to calculating gene expression values, we can use the RNA-seq reads to look for SNPs or RNA editing in the transcriptome:
{{{
    >> mutations = find_snps(reads)
    mutations =
          SNPs: {1x4 cell}
          Meta: [1x1 struct]
}}}

The analysis will take hours to run, but once it's done, you can print the discovered SNPs for the first sample with the following function:
{{{
    >> print_snps(mutations.SNPs{1})
}}}

The function automatically orders the SNPs so that the SNPs with the highest prevalence and highest number of reads are placed at the head of the list. 

Finally, we can use the RNA-seq reads to discover fusion genes and alternative splicing events:
{{{
    >> rearrangements = find_rearrangements(reads, 20)
    rearrangements =
          TagFusions: {1x4 cell}
             Fusions: {1x4 cell}
        AltSplicings: {1x4 cell}
                Meta: [1x1 struct]
}}}

The second parameter specifies the size of the start and end tags which will be extracted from all reads and mapped against the exome to discover novel junctions. The resulting data structure contains fusion gene candidates obtained with different levels of validation, and alternative splicing events (only exon deletions for now). If you want to pool rearrangements from multiple samples to increase the resolution, you can do it in the following way:
{{{
    >> pooled_fusions = pool_rearrangements(rearrangements.Fusions(1:4));
}}}

You can then print the pooled set of fusions (this can lead to a lot of output, so use of the Matlab diary() function is adviced):
{{{
    >> print_fusions(pooled_fusions);
}}}

Similarly you can print the list of alternative splicing events:
{{{
    >> print_alt_splicings(rearrangements.AltSplicings{1});
}}}

These commands report the fused exons and show which transcript and gene they normally belong to. The output also shows the number of reads that supports a particular exon-exon junction, and the sequences of the supporting RNA-seq reads.






= MICRORNA SEQUENCING ANALYSIS =

In this chapter we will look at running some microRNA analyses using small RNA sequencing data. First, let's load up some data:
{{{
    >> reads = realize(query('Wei GBM/small RNAseq reads'));
}}}

Now we can calculate expression levels for both pre-miRNA and mature miRNA transcripts by using the following function:
{{{
    >> mirna_expr = mirna_expression_rnaseq(reads);
    mirna_expr =
          miRNA: [904x10 double]
      pre_miRNA: [721x10 double]
           Meta: [1x1 struct]
}}}

The function has a number of optional arguments, and you can see documentation for these arguments by typing "help mirna_expression_rnaseq". The second argument specifies the maximum number of allowed nucleotide mismatches when aligning the reads against miRNA and pre-miRNA sequences. The output data structure contains two matrices, one for mature miRNA and one for pre-miRNA expression levels. The matrix rows can be identified using the organism attributes "organism.miRNA.Name" and "organism.pre_miRNA.Name".

Next, we can look for miRNA mutations and length polymorphisms using the following function:
{{{
    >> mutations = mirna_mutations(reads, 'ACGCTTCC');
    mutations =
          SNPs: {1x10 cell}
      Isoforms: {904x10 cell}
          Meta: [1x1 struct]
}}}

The function assumes that the small RNA sequencing was performed by first ligating a 3' adapter to all small RNA transcripts. The adapter sequence must be given as the second argument to the function. The presence of this adapter allows the function to computationally look for length polymorphisms in microRNA transcripts.

Now, you can print a list of the discovered microRNA SNPs in the same way as with whole transcriptome sequencing:
{{{
    >> find_snps(mutations.SNPs{1});
    SNPs ordered by significance score:
    - hsa-miR-1274a:11:A>G: 274/304 (90.1%)
    - hsa-miR-1260:9:T>G: 1792/2020 (88.7%)
    - hsa-miR-376a:6:A>G: 942/1085 (86.8%)
	              ...
}}}





= DATA IMPORT - GENERAL =

In the previous chapters of this tutorial, we have retrieved all of our data directly from the pipeline data store, in a format that is immediately usable in Matlab. In real life, bioinformatics data is stored in a number of different formats and representations. In order to analyze real life data, the data must first be brought into the pipeline and represented in the unified data formats that the pipeline deals with.

To import microarray data into the pipeline, you can use the following command:
{{{
    >> import_uarray_data('dataset name', 'Affymetrix Human Exon 1.0 ST')
    Importing GSM526134_YX_Exon1_PCA0001.CEL...
    Importing GSM526135_YX_Exon1_PCA0002.CEL...
    Importing GSM526136_YX_Exon1_PCA0003.CEL...
                   ...
}}}

This function checks the current working directory for microarray sample files, and reads them into the pipeline, creating a new dataset with the requested name. You also need to specify the microarray platform, since the function cannot yet autodetect all microarray platforms based on the sample file contents. The second argument will likely become optional in the future.

You can get a list of supported microarray platform names from the pipeline's internal ontologies:
{{{
    >> ontologies.uarray_platforms
}}}

To import sequencing data into the pipeline, you can use the command:
{{{
    >> import_seq_reads('dataset name', 'ABI SOLiD V3')
}}}

Again, the command looks for FASTA, FASTQ and colorspace read files in the current working directory, and creates a new dataset out of them. This function will also move the read files into the pipeline data store, so be careful.

A list of sequencing platform names is again provided in the internal ontologies:
{{{
    >> ontologies.sequencing_platforms
}}}

If you call import_uarray_data() or import_seq_reads() as shown above, no metadata will be associated with the samples. The only populated metadata columns will be Meta.Sample.Filename, Meta.Platform and Meta.Type.

To add metadata to any dataset, you must first create or otherwise acquire an Excel spreadsheet containing the clinical metadata for your sample. Then, once you have an Excel spreadsheet, you can use the function `augment_meta_xls()` to associate metadata with your samples:
{{{
    >> augment_meta_xls('dataset name', 'spreadsheet.xls')
}}}

Note that the spreadsheet must be in the old `.xls` format. The newer `.xlsx` is not supported by Matlab. If you have any issues importing metadata from the `.xls` format, you can also opt to use the function `augment_meta_tab()`, which uses tab delimited text files rather than `.xls` files. Excel supports exporting your spreadsheet as a tab delimited text file.

The metadata association process first looks at your spreadsheet and tries to find a column with the name "Filename". If such a column is found, clinical metadata is associated with samples based on the sample filenames stored in Meta.Sample.Filename. The association can also happen through the field Meta.Sample.ID, if a filename-to-sample mapping is provided in the following manner:
{{{
    >> sample_map = containers.Map;
    >> sample_map('filename') = 'sample ID';   % Construct the filename-sample mapping
    >> ...
    >> augment_meta_xls('dataset name', 'metadata.xls', 'SampleMap', sample_map)
}}}







= DATA IMPORT - TCGA =

The pipeline supports automated importing of TCGA data, given that your TCGA data is stored in a directory structure similar or identical to that used by the TCGA FTP and HTTPS directories. All you need to do is to enter a directory that contains one or more TCGA archives, and run the function import_tcga_uarray_data(). Here's an example:
{{{
    >> cd /worktmp/TCGA/gbm/cgcc/mskcc.org/hg-cgh-244a/cna
    >> import_tcga_uarray_data('tcga/gbm agilent cgh 244a mskcc test', ...
           'Agilent HG CGH 244A');
}}}
    
The function will automatically scan the directory and any subdirectories for SDRF files, and will then look for raw data files for the samples described in the SDRF files. The function then calls import_uarray_data() to import the microarray data into the pipeline. As the final step, TCGA patient metadata is automatically associated with the microarray samples.





= DATA IMPORT - GENE EXPRESSION OMNIBUS (GEO) =

Limited support is provided for importing data from the Gene Expression Omnibus service. Raw microarray samples downloaded from GEO can be automatically imported using the generic import_uarray_data(). For metadata, you can download a series matrix file from GEO and then parse it:
{{{
    >> geo_meta = read_geo_series_matrix('GSE21032_series_matrix.txt')
    geo_meta =
                Title: 'Integrative genomic profiling of human prostate cancer'
      SeriesAccession: 'GSE21032'
       SubmissionDate: 'Mar 23 2010'
       LastUpdateDate: 'Jul 06 2010'
          SampleTitle: {185x1 cell}
      SampleAccession: {185x1 cell}
             SampleID: {185x1 cell}
}}}

For GEO datasets, you can often use the !SampleAccession and SampleID fields to build a mapping from filenames to sample IDs. This mapping can then be given to the function import_uarray_data() as the optional parameter '!SampleMap'. This allows you to create a spreadsheet that only contains clinical metadata associated with sample IDs, and the association with filenames then happens automatically via the mapping.